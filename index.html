<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model">
  <meta name="keywords" content="3D Part Segmentation, Reasoning, vision-language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PARIS3D</title>

  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container ">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/AmrinKareem">Amrin Kareem</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LsivLPoAAAAJ&hl=en">Jean Lahoud</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bZ3YBRcAAAAJ&hl=en">Hisham Cholakkal</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Mohamed bin Zayed University of Artificial Intelligence, UAE</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.03836"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=lfG6luo0_JY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AmrinKareem/PARIS3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="./static/PARIS3D.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-desktop"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container ">
    <div class="hero-body">
      <img src="./static/images/introimage.png"
                 class="teaser-image"
                 alt="teaser image."/>

      <h2 class="subtitle has-text-centered">
        We propose PARIS3D, a model that is capable of segmenting parts of 3D objects based on implicit textual queries and generating natural language explanations corresponding to 3D object segmentation requests. The figure shows our semantic segmentation results.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in 3D perception systems have significantly improved their ability to perform visual recognition tasks such as segmentation. However, these systems still heavily rely on explicit human instruction to identify target objects or categories, lacking the capability to actively reason and comprehend implicit user intentions. We introduce a novel segmentation task known as reasoning part segmentation for 3D objects, aiming to output a segmentation mask based on complex and implicit textual queries about specific parts of a 3D object.  To facilitate evaluation and benchmarking, we present a large 3D dataset comprising over 60k instructions paired with corresponding ground-truth part segmentation annotations specifically curated for reasoning-based 3D part segmentation.  We propose a model that is capable of segmenting parts of 3D objects based on implicit textual queries and generating natural language explanations corresponding to 3D object segmentation requests. Experiments show that our method achieves competitive performance to models that use explicit queries, with the additional abilities to identify part concepts, reason about them, and complement them with world knowledge.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/lfG6luo0_JY?si=O8QWUf5c9Ztogb47" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Architecture</h2>
        <img src="./static/images/architecturefin.png" class="pipeline" alt=""/>
      </div>
    </div>
</section>  

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Semantic Segmentation Results</h2>
        <img src="./static/images/results.png" class="instance_seg" alt="" width="100%"/>
      </div>
      <p>
        Comparison to previous 3D part segmentation methods. Object category mIoU(%) is shown. In the 45x8+28k setting, baseline models use an additional 28k training shapes for 17 overlapping object categories. These are categories present in common with PartNet datset. For the remaining 28 non-overlapping object categories, there are only 8 shapes per object category during training. PartSLIP* indicates that one model has been trained for each category. + shows our implementation of PartSLIP where one model is trained for all the categories together.
     </p>
    </div>
</section>  

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Generalizability Experiments</h2>
        <img src="./static/images/realpc.drawio.png" class="realworld" alt="" width="100%"/>
      </div>
      <p>
       To demonstrate the generalizability of PARIS3D to data derived from the real world, we perform our 3D segmentation on real point clouds shot using a smartphone's LiDAR sensor, as suggested by <a
              href="https://github.com/Colin97/PartSLIP_page">this paper</a>. In the image, we show qualitative examples of passing the fused point clouds through the PARIS3D architecture to obtain part segmentation labels as in the previous experiments without much drop in performance.
     </p>
    </div>
</section>  

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">RPSeg Dataset</h2>
        <img src="./static/images/traindata.png" class="quantitative_comparison" alt="" width="100%"/>
      </div>
      <p>
        Given a coloured point cloud P, the goal of a 3D segmentation model is to predict its label for each point. However, in our reasoning segmentation task, we go further to output a 3D segmentation mask M, given an input point cloud and an implicit query text instruction. The complexity of the query text in reasoning part segmentation is a key differentiator. Instead of providing the names of the parts, the query text may include more intricate expressions that involve an understanding of structures, geometries, and semantics of 3D objects. By introducing this task, we aim to bridge the gap between user intent and system response, enabling more intuitive and dynamic interactions in 3D object perception.
     </p>
    </div>
</section>  


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> @misc{kareem2024paris3d,
      title={PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model}, 
      author={Amrin Kareem and Jean Lahoud and Hisham Cholakkal},
      year={2024},
      eprint={2404.03836},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2404.03836">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/AmrinKareem/PARIS3D" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
