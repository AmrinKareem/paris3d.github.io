<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model">
  <meta name="keywords" content="3D Part Segmentation, Reasoning, vision-language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PartSLIP</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container ">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/AmrinKareem">Amrin Kareem</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LsivLPoAAAAJ&hl=en">Dr. Jean Lahoud</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bZ3YBRcAAAAJ&hl=en">Dr. Hisham Cholakkal</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/aJMQCJJObzU?feature=shared"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AmrinKareem/PARIS3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="./static/slides.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-desktop"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container ">
    <div class="hero-body">
      <img src="./static/images/redintro.png"
                 class="teaser-image"
                 alt="teaser image."/>

      <h2 class="subtitle has-text-centered">
        We propose PARIS3D, a model that is capable of segmenting parts of 3D objects based on implicit textual queries and generating natural language explanations corresponding to 3D object segmentation requests. The figure shows our semantic segmentation results.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in 3D perception systems have significantly improved their ability to perform visual recognition tasks such as segmentation. However, these systems still heavily rely on explicit human instruction to identify target objects or categories, lacking the capability to actively reason and comprehend implicit user intentions. We introduce a novel segmentation task known as reasoning part segmentation for 3D objects, aiming to output a segmentation mask based on complex and implicit textual queries about specific parts of a 3D object.  To facilitate evaluation and benchmarking, we present a large 3D dataset comprising over 60k instructions paired with corresponding ground-truth part segmentation annotations specifically curated for reasoning-based 3D part segmentation.  We propose a model that is capable of segmenting parts of 3D objects based on implicit textual queries and generating natural language explanations corresponding to 3D object segmentation requests. Experiments show that our method achieves competitive performance to models that use explicit queries, with the additional abilities to identify part concepts, reason about them, and complement them with world knowledge.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/VGORtR2mJog"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Overall Pipeline</h2>
        <img src="./static/images/architecturefin.png" class="pipeline" alt=""/>
      </div>
    </div>
</section>  

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Semantic Segmentation Results</h2>
        <img src="./static/images/results.png" class="instance_seg" alt="" width="60%"/>
      </div>
      <p>
        PARIS3D results on the RPSeg dataset. Different part instances are in different colors.
     </p>
    </div>
</section>  

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Real-World Point Clouds</h2>
        <img src="./static/images/realpc.drawio.png" class="realworld" alt="" width="100%"/>
      </div>
      <p>
        PartSLIP can be directly applied to real-world point clouds without encountering significant domain gap: iPhone-scanned point clouds (first row), text prompts (second row), and PartSLIP (8-shot) results (third row).
     </p>
    </div>
</section>  

<section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="is-centered">
        <h2 class="title is-3">Quantitative Comparison</h2>
        <img src="./static/images/quantitative_comparison.png" class="quantitative_comparison" alt="" width="100%"/>
      </div>
      <p>
        PartSLIP achieves impressive zero-shot performances, and few-shot results are highly competitive compared to the fully supervised counterparts.
     </p>
    </div>
</section>  


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> @inproceedings{liu2023partslip,
        title={Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models},
        author={Liu, Minghua and Zhu, Yinhao and Cai, Hong and Han, Shizhong and Ling, Zhan and Porikli, Fatih and Su, Hao},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={21736--21746},
        year={2023}
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2212.01558.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Colin97/PartSLIP" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
